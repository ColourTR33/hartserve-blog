<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Enterprise AI Design Patterns: A Field Guide for Architects | Mark Hartley</title><meta name=keywords content="RAG,LLMOps,Security,Token Optimization,Architecture"><meta name=description content="Practical patterns for enterprise AI: RAG, cost control, security, LLMOps, and change management—with templates and pitfalls."><meta name=author content="Mark Hartley"><link rel=canonical href=https://mark.thehartleys.uk/blog/posts/enterprise-ai-2/><link crossorigin=anonymous href=https://mark.thehartleys.uk/blog/assets/css/stylesheet.a090830a421002426baafbd314e38f149d77b4c48a12ee9312700d770b27fb26.css integrity="sha256-oJCDCkIQAkJrqvvTFOOPFJ13tMSKEu6TEnANdwsn+yY=" rel="preload stylesheet" as=style><link rel=icon href=https://mark.thehartleys.uk/blog/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://mark.thehartleys.uk/blog/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://mark.thehartleys.uk/blog/favicon-32x32.png><link rel=apple-touch-icon href=https://mark.thehartleys.uk/blog/apple-touch-icon.png><link rel=mask-icon href=https://mark.thehartleys.uk/blog/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://mark.thehartleys.uk/blog/posts/enterprise-ai-2/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://mark.thehartleys.uk/blog/posts/enterprise-ai-2/"><meta property="og:site_name" content="Mark Hartley"><meta property="og:title" content="Enterprise AI Design Patterns: A Field Guide for Architects"><meta property="og:description" content="Practical patterns for enterprise AI: RAG, cost control, security, LLMOps, and change management—with templates and pitfalls."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-08-30T00:00:00+00:00"><meta property="article:modified_time" content="2025-08-30T00:00:00+00:00"><meta property="og:image" content="https://mark.thehartleys.uk/posts/enterprise-ai-2/hero.jpg"><meta property="og:see_also" content="https://mark.thehartleys.uk/blog/posts/enterprise-ai-1/"><meta property="og:see_also" content="https://mark.thehartleys.uk/blog/posts/enterprise-ai-3/"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://mark.thehartleys.uk/posts/enterprise-ai-2/hero.jpg"><meta name=twitter:title content="Enterprise AI Design Patterns: A Field Guide for Architects"><meta name=twitter:description content="Practical patterns for enterprise AI: RAG, cost control, security, LLMOps, and change management—with templates and pitfalls."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://mark.thehartleys.uk/blog/posts/"},{"@type":"ListItem","position":2,"name":"Enterprise AI Design Patterns: A Field Guide for Architects","item":"https://mark.thehartleys.uk/blog/posts/enterprise-ai-2/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Enterprise AI Design Patterns: A Field Guide for Architects","name":"Enterprise AI Design Patterns: A Field Guide for Architects","description":"Practical patterns for enterprise AI: RAG, cost control, security, LLMOps, and change management—with templates and pitfalls.","keywords":["RAG","LLMOps","Security","Token Optimization","Architecture"],"articleBody":"Enterprise AI Design Patterns Executive Summary Generative AI is not just another software deployment; it is the integration of a non-deterministic, reasoning workforce into existing business processes. Success hinges on more than technical acumen. It requires a strategic approach to grounding models in enterprise data (Context), managing their unpredictable nature, optimizing for cost (Tokens), and securing the new interfaces they create. This series of posts guides architects in navigating these complexities to deliver tangible business value.\nAI Functions \u0026 Applications Part 1 - Foundational AI Concepts - Part 2: Architectural Patterns \u0026 Implementation Part 3: Enterprise-Grade Considerations Part 2: Architectural Patterns \u0026 Implementation RAG: Grounding AI in Enterprise Reality Retrieval-Augmented Generation (RAG) is the single most important architectural pattern for enterprise AI. It allows LLMs to answer questions based on specific, private company data without needing to be retrained. It works like an “open-book exam” for the AI and makes the difference between faithfulness and relevancy to help avoid hallucinated synthesis.\nHow to Implement RAG: Ingestion \u0026 Chunking: Source documents (PDFs, docs, web pages) are broken down into smaller, manageable chunks of text.\nEmbedding \u0026 Indexing: Each chunk is converted into a numerical representation (an embedding) using an embedding model. These embeddings are stored in a specialized Vector Database. The embedding captures the semantic meaning of the text.\nRetrieval: When a user asks a query, the query is also converted into an embedding. The system then searches the vector database for the text chunks with the most similar embeddings (i.e., the most relevant information).\nAugmentation \u0026 Generation: The retrieved text chunks (the Context) are inserted into a prompt along with the user’s original Query. This complete prompt is sent to the LLM, which generates an answer based only on the provided information.\nConsiderations: Chunking Strategy: The size and overlap of chunks dramatically affect retrieval quality. Too small, and you lose context; too large, and you introduce noise.\nVector Database Selection: Choose a database that can handle the client’s scale and security requirements (e.g., Pinecone, Weaviate, or managed services from cloud providers).\nRetrieval Quality: The success of RAG depends almost entirely on retrieving the right context. Use hybrid search (keyword + semantic) and re-ranking models to improve relevance.\nData Freshness: Implement a process to keep the vector database in sync with the source documents.\nRAG Go-Live Checklist\nChunk size \u0026 overlap tuned with evaluation set\nHybrid search (BM25 + vector)\nReranker or MMR configured\nRetrieval filtered by user ACL\nPrompt window budgeted (context + system + user)\nEval: answer faithfulness \u0026 context coverage\nRedaction filter: PII/PCI before display\nStrategic Token Optimisation Optimising token usage can cut costs by 2-300%, however, it is not just about saving money; it’s about improving latency and response quality.\nQuality Prompts (Prompt Engineering): Be Concise and Specific: Remove filler words. Use clear, unambiguous language.\nFew-Shot Prompting: Provide 2-3 examples of the desired input and output format within the prompt. This guides the model better than a lengthy explanation.\nPre-formatting and Pre-processing: Don’t send raw data to the model. Clean and summarize context before inserting it into the prompt. Remove HTML tags, boilerplate text, and irrelevant sections.\nDesign Pattern: LLM Cascade: Use a chain of models for complex tasks. A cheaper, faster model (e.g., Gemini Flash, GPT-3.5 Turbo) can perform initial classification, routing, or summarization. If the task is complex, it can be escalated to a more powerful, expensive model (e.g., Gemini Advanced, GPT-4).\nRecord Token Usage: Centralized API Gateway: Route all AI calls through a central gateway. This is the ideal place to log every request and response.\nLogging Metadata: For each call, log the timestamp, user/role ID, application name, model used, prompt tokens, completion tokens, and total tokens.\nDashboarding: Feed this data into a monitoring dashboard (e.g., Datadog, Grafana) to visualize costs by user, department, and application. This is critical for chargeback models and identifying misuse.\nManaging Long-Term Memory: Context Save Points An LLM’s context window is finite. In a long conversation, early information is forgotten. “Context save points” are a pattern to overcome this.\nHow it Works: After a few conversational turns, use an LLM to create a concise summary of the conversation so far.\nImplementation: In the next turn, instead of including the full, verbose chat history in the prompt, include only the summary of the previous turns plus the most recent 1-2 exchanges. This keeps the “memory” of the conversation alive without exceeding the token limit.\nUse Cases: Essential for multi-turn conversational agents, chatbots, and co-pilots that assist with complex, multi-step tasks.\n","wordCount":"750","inLanguage":"en","image":"https://mark.thehartleys.uk/posts/enterprise-ai-2/hero.jpg","datePublished":"2025-08-30T00:00:00Z","dateModified":"2025-08-30T00:00:00Z","author":{"@type":"Person","name":"Mark Hartley"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://mark.thehartleys.uk/blog/posts/enterprise-ai-2/"},"publisher":{"@type":"Organization","name":"Mark Hartley","logo":{"@type":"ImageObject","url":"https://mark.thehartleys.uk/blog/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://mark.thehartleys.uk/blog/ accesskey=h title="Mark Hartley (Alt + H)">Mark Hartley</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://mark.thehartleys.uk/blog/posts/ title="Other Posts"><span>Other Posts</span></a></li><li><a href=https://mark.thehartleys.uk/blog/series/enterprise-ai-design-patterns/ title=Series><span>Series</span></a></li><li><a href=https://mark.thehartleys.uk/blog/cv/mark-hartley-cv.pdf title="Curriculum Vitae"><span>Curriculum Vitae</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://mark.thehartleys.uk/blog/>Home</a>&nbsp;»&nbsp;<a href=https://mark.thehartleys.uk/blog/posts/>Posts</a></div><h1 class=post-title>Enterprise AI Design Patterns: A Field Guide for Architects</h1><div class=post-description>Practical patterns for enterprise AI: RAG, cost control, security, LLMOps, and change management—with templates and pitfalls.</div><div class=post-meta><span title='2025-08-30 00:00:00 +0000 UTC'>August 30, 2025</span>&nbsp;·&nbsp;<span>4 min</span>&nbsp;·&nbsp;<span>Mark Hartley</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#enterprise-ai-design-patterns aria-label="Enterprise AI Design Patterns">Enterprise AI Design Patterns</a><ul><li><a href=#executive-summary aria-label="Executive Summary">Executive Summary</a></li><li><a href=#ai-functions--applications aria-label="AI Functions & Applications">AI Functions & Applications</a></li><li><a href=#part-2-architectural-patterns--implementation aria-label="Part 2: Architectural Patterns & Implementation">Part 2: Architectural Patterns & Implementation</a><ul><li><a href=#rag-grounding-ai-in-enterprise-reality aria-label="RAG: Grounding AI in Enterprise Reality">RAG: Grounding AI in Enterprise Reality</a><ul><ul><li><a href=#how-to-implement-rag aria-label="How to Implement RAG:">How to Implement RAG:</a></li><li><a href=#considerations aria-label=Considerations:>Considerations:</a></li></ul></ul></li><li><a href=#strategic-token-optimisation aria-label="Strategic Token Optimisation">Strategic Token Optimisation</a><ul><li><a href=#quality-prompts-prompt-engineering aria-label="Quality Prompts (Prompt Engineering):">Quality Prompts (Prompt Engineering):</a></li><li><a href=#pre-formatting-and-pre-processing aria-label="Pre-formatting and Pre-processing:">Pre-formatting and Pre-processing:</a></li><li><a href=#design-pattern-llm-cascade aria-label="Design Pattern: LLM Cascade:">Design Pattern: LLM Cascade:</a></li><li><a href=#record-token-usage aria-label="Record Token Usage:">Record Token Usage:</a></li></ul></li><li><a href=#managing-long-term-memory-context-save-points aria-label="Managing Long-Term Memory: Context Save Points">Managing Long-Term Memory: Context Save Points</a></li></ul></li></ul></li></ul></div></details></div><div class=post-content><h1 id=enterprise-ai-design-patterns>Enterprise AI Design Patterns</h1><h2 id=executive-summary>Executive Summary</h2><p>Generative AI is not just another software deployment; it is the integration of a non-deterministic, reasoning workforce into existing business processes. Success hinges on more than technical acumen. It requires a strategic approach to grounding models in enterprise data (Context), managing their unpredictable nature, optimizing for cost (Tokens), and securing the new interfaces they create. This series of posts guides architects in navigating these complexities to deliver tangible business value.</p><h2 id=ai-functions--applications>AI Functions & Applications</h2><ul><li>Part 1 - Foundational AI Concepts
<strong>- Part 2: Architectural Patterns & Implementation</strong></li><li>Part 3: Enterprise-Grade Considerations</li></ul><h2 id=part-2-architectural-patterns--implementation>Part 2: Architectural Patterns & Implementation</h2><h3 id=rag-grounding-ai-in-enterprise-reality>RAG: Grounding AI in Enterprise Reality</h3><p><strong>Retrieval-Augmented Generation (RAG)</strong> is the single most important architectural pattern for enterprise AI. It allows LLMs to answer questions based on specific, private company data without needing to be retrained. It works like an &ldquo;open-book exam&rdquo; for the AI and makes the difference between faithfulness and relevancy to help avoid hallucinated synthesis.</p><h5 id=how-to-implement-rag>How to Implement RAG:</h5><ol><li><p><strong>Ingestion & Chunking:</strong> Source documents (PDFs, docs, web pages) are broken down into smaller, manageable chunks of text.</p></li><li><p><strong>Embedding & Indexing:</strong> Each chunk is converted into a numerical representation (an embedding) using an embedding model. These embeddings are stored in a specialized Vector Database. The embedding captures the semantic meaning of the text.</p></li><li><p><strong>Retrieval:</strong> When a user asks a query, the query is also converted into an embedding. The system then searches the vector database for the text chunks with the most similar embeddings (i.e., the most relevant information).</p></li><li><p><strong>Augmentation & Generation:</strong> The retrieved text chunks (the Context) are inserted into a prompt along with the user&rsquo;s original Query. This complete prompt is sent to the LLM, which generates an answer based only on the provided information.</p></li></ol><h5 id=considerations>Considerations:</h5><ul><li><p><strong>Chunking Strategy:</strong> The size and overlap of chunks dramatically affect retrieval quality. Too small, and you lose context; too large, and you introduce noise.</p></li><li><p><strong>Vector Database Selection:</strong> Choose a database that can handle the client&rsquo;s scale and security requirements (e.g., Pinecone, Weaviate, or managed services from cloud providers).</p></li><li><p><strong>Retrieval Quality:</strong> The success of RAG depends almost entirely on retrieving the right context. Use hybrid search (keyword + semantic) and re-ranking models to improve relevance.</p></li><li><p><strong>Data Freshness:</strong> Implement a process to keep the vector database in sync with the source documents.</p></li></ul><p><strong>RAG Go-Live Checklist</strong></p><ul><li><p>Chunk size & overlap tuned with evaluation set</p></li><li><p>Hybrid search (BM25 + vector)</p></li><li><p>Reranker or MMR configured</p></li><li><p>Retrieval filtered by user ACL</p></li><li><p>Prompt window budgeted (context + system + user)</p></li><li><p>Eval: answer faithfulness & context coverage</p></li><li><p>Redaction filter: PII/PCI before display</p></li></ul><h3 id=strategic-token-optimisation>Strategic Token Optimisation</h3><p>Optimising token usage can cut costs by 2-300%, however, it is not just about saving money; it&rsquo;s about improving latency and response quality.</p><h4 id=quality-prompts-prompt-engineering>Quality Prompts (Prompt Engineering):</h4><ul><li><p><strong>Be Concise and Specific:</strong> Remove filler words. Use clear, unambiguous language.</p></li><li><p><strong>Few-Shot Prompting:</strong> Provide 2-3 examples of the desired input and output format within the prompt. This guides the model better than a lengthy explanation.</p></li></ul><h4 id=pre-formatting-and-pre-processing>Pre-formatting and Pre-processing:</h4><p>Don&rsquo;t send raw data to the model. Clean and summarize context before inserting it into the prompt. Remove HTML tags, boilerplate text, and irrelevant sections.</p><h4 id=design-pattern-llm-cascade>Design Pattern: LLM Cascade:</h4><p>Use a chain of models for complex tasks. A cheaper, faster model (e.g., Gemini Flash, GPT-3.5 Turbo) can perform initial classification, routing, or summarization. If the task is complex, it can be escalated to a more powerful, expensive model (e.g., Gemini Advanced, GPT-4).</p><h4 id=record-token-usage>Record Token Usage:</h4><p><strong>Centralized API Gateway:</strong> Route all AI calls through a central gateway. This is the ideal place to log every request and response.</p><p><strong>Logging Metadata:</strong> For each call, log the timestamp, user/role ID, application name, model used, prompt tokens, completion tokens, and total tokens.</p><p><strong>Dashboarding:</strong> Feed this data into a monitoring dashboard (e.g., Datadog, Grafana) to visualize costs by user, department, and application. This is critical for chargeback models and identifying misuse.</p><h3 id=managing-long-term-memory-context-save-points>Managing Long-Term Memory: Context Save Points</h3><p>An LLM&rsquo;s context window is finite. In a long conversation, early information is forgotten. &ldquo;Context save points&rdquo; are a pattern to overcome this.</p><p><strong>How it Works:</strong> After a few conversational turns, use an LLM to create a concise summary of the conversation so far.</p><p><strong>Implementation:</strong> In the next turn, instead of including the full, verbose chat history in the prompt, include only the summary of the previous turns plus the most recent 1-2 exchanges. This keeps the &ldquo;memory&rdquo; of the conversation alive without exceeding the token limit.</p><p><strong>Use Cases:</strong> Essential for multi-turn conversational agents, chatbots, and co-pilots that assist with complex, multi-step tasks.</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://mark.thehartleys.uk/blog/posts/enterprise-ai-1/><span class=title>« Prev</span><br><span>Enterprise AI Design Patterns: A Field Guide for Architects</span>
</a><a class=next href=https://mark.thehartleys.uk/blog/posts/enterprise-ai-3/><span class=title>Next »</span><br><span>Enterprise AI Design Patterns: A Field Guide for Architects</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://mark.thehartleys.uk/blog/>Mark Hartley</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>